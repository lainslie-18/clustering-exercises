{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9f8517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "from math import sqrt\n",
    "\n",
    "from pydataset import data\n",
    "\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score, classification_report,confusion_matrix, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.feature_selection import f_regression \n",
    "\n",
    "import graphviz\n",
    "from graphviz import Graph\n",
    "\n",
    "import env\n",
    "import acquire\n",
    "import prepare\n",
    "import os\n",
    "\n",
    "# turn off pink boxes for demo\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfa5a9a",
   "metadata": {},
   "source": [
    "# Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d87676",
   "metadata": {},
   "source": [
    "**The goal** of this stage is to clearly define your goal(s), measures of success, and plans on how to achieve that.\n",
    "\n",
    "**The deliverable** is documentation of your goal, your measure of success, and how you plan on getting there.\n",
    "\n",
    "**How to get there:** You can get there by answering questions about the final product & formulating or identifying any initial hypotheses (from you or others).\n",
    "\n",
    "**Common questions include:**\n",
    "- What will the end product look like?\n",
    "- What format will it be in?\n",
    "- Who will it be delivered to?\n",
    "- How will it be used?\n",
    "- How will I know I'm done?\n",
    "- What is my MVP?\n",
    "- How will I know it's good enough?\n",
    "\n",
    "\n",
    "**Formulating hypotheses**\n",
    "- Is attribute V1 related to attribute V2?\n",
    "- Is the mean of target variable Y for subset A significantly different from that of subset B?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020d86ac",
   "metadata": {},
   "source": [
    "# Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa5cc7b",
   "metadata": {},
   "source": [
    "**The goal** is to create a path from original data sources to the environment in which you will work with the data. You will gather data from sources in order to prepare and clean it in the next step.\n",
    "\n",
    "**The deliverable** is a file, acquire.py, that contains the function(s) needed to reproduce the acquisition of data.\n",
    "\n",
    "**How to get there:**\n",
    "\n",
    "- If the data source is SQL, you may need to do some clean-up, integration, aggregation or other manipulation of data in the SQL environment before reading the data into your python environment.\n",
    "- Using the Python library pandas, acquire the data into a dataframe using a function that reads from your source type, such as pandas.read_csv for acquiring data from a csv.\n",
    "- You may use Spark and/or Hive when acquiring data from a distributed environment, such as HDFS.\n",
    "Examples of source types include RDBMS, NoSQL, HDFS, Cloud Files (S3, google drive), static local flat files (csv, txt, xlsx)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d27e4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_connection(db, user=env.user, host=env.host, password=env.password):\n",
    "    '''\n",
    "    This function takes in user credentials from an env.py file and a database name and creates a connection to the Codeup database through a connection string \n",
    "    '''\n",
    "    return f'mysql+pymysql://{user}:{password}@{host}/{db}'\n",
    "    \n",
    "    zillow_sql_query =  '''\n",
    "                    select *\n",
    "                    from properties_2017\n",
    "                    join predictions_2017 using(parcelid)\n",
    "                    join propertylandusetype using(propertylandusetypeid)\n",
    "                    where propertylandusedesc = 'Single Family Residential'\n",
    "                    and transactiondate like '2017%%';\n",
    "                    '''\n",
    "    \n",
    "def query_zillow_data():\n",
    "    '''\n",
    "    This function uses the get_connection function to connect to the zillow database and returns the zillow_sql_query read into a pandas dataframe\n",
    "    '''\n",
    "    return pd.read_sql(zillow_sql_query,get_connection('zillow'))\n",
    "\n",
    "\n",
    "def get_zillow_data():\n",
    "    '''\n",
    "    This function checks for a local zillow.csv file and reads it into a pandas dataframe, if it exists. If not, it uses the get_connection & query_zillow_data functions to query the data and write it locally to a csv file\n",
    "    '''\n",
    "    # If csv file exists locally, read in data from csv file.\n",
    "    if os.path.isfile('zillow.csv'):\n",
    "        df = pd.read_csv('zillow.csv', index_col=0)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # Query and read data from zillow database\n",
    "        df = query_zillow_data()\n",
    "        \n",
    "        # Cache data\n",
    "        df.to_csv('zillow.csv')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ce71b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_clipboard\n",
    "pd.read_excel\n",
    "pd.read_csv\n",
    "pd."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbf09c2",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a9bb57",
   "metadata": {},
   "source": [
    "**The goal** is to have data, split into 3 samples (train, validate, and test), in a format that can easily be explored, analyzed and visualized. \n",
    "\n",
    "**The deliverable** is a file, prep.py, that contains the function(s) needed to reproduce the preparation of the data.\n",
    "\n",
    "**How to get there:**\n",
    "\n",
    "- Python libraries: pandas, matplotlib, seaborn, scikit-learn.\n",
    "- Use pandas to perform tasks such as handling null values, outliers, normalizing text, binning of data, changing data types, etc.\n",
    "- Use matplotlib or seaborn to plot distributions of numeric attributes and target.\n",
    "- Use scikit-learn to split the data into train and test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29f87f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df.columns\n",
    "df.index\n",
    "df.shape\n",
    "df.describe().T\n",
    "df.info()\n",
    "df.dtypes\n",
    "# document takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51c3491",
   "metadata": {},
   "source": [
    "## Step 1: Remove unwanted observations (duplicate, irrelevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7cfa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['column1', 'column2'])\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7647618e",
   "metadata": {},
   "source": [
    "## Step 2: Tidy data\n",
    "* change data types, correct and standardize text,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39125345",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[col].astype(int)\n",
    "df = df.replace(r'^\\s*$', np.nan, regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094dd3ba",
   "metadata": {},
   "source": [
    "## Step 3: Take Care of Outliers\n",
    "* ignore, drop rows, snap to a selected max/min value, create bins (cut, qcut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7201baed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, k, col_list):\n",
    "    ''' remove outliers from a list of columns in a dataframe \n",
    "        and return that dataframe\n",
    "    '''\n",
    "    for col in col_list:\n",
    "        q1, q3 = df[col].quantile([.25, .75])  # get quartiles\n",
    "        iqr = q3 - q1   # calculate interquartile range\n",
    "        \n",
    "        upper_bound = q3 + k * iqr   # get upper bound\n",
    "        lower_bound = q1 - k * iqr   # get lower bound\n",
    "\n",
    "        # return dataframe without outliers\n",
    "        df = df[(df[col] > lower_bound) & (df[col] < upper_bound)]\n",
    "        \n",
    "    return df\n",
    "\n",
    "df = remove_outliers(df, 1.5, ['col1', 'col2', 'col3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020f9ff8",
   "metadata": {},
   "source": [
    "## Step 4: Handle Missing Values\n",
    "* drop columns or rows with missing values\n",
    "* fill with zero or other value where it makes sense\n",
    "* impute values (must be done on split data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79059cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for and handle nulls\n",
    "df.isnull().sum()\n",
    "df[df.exam1.isna()]\n",
    "df.isnull().any()\n",
    "df.fillna(value='value')\n",
    "# How many nulls have in each row?\n",
    "df.isnull().sum(axis =1).value_counts()\n",
    "# impute \n",
    "imputer = SimpleImputer(missing_values = None, strategy='most_frequent') # mean, median, or most frequent\n",
    "imputer = imputer.fit(train[['col']])\n",
    "train[['col']] = imputer.transform(train[['col']])\n",
    "validate[['col']] = imputer.transform(validate[['col']])\n",
    "test[['col']] = imputer.transform(test[['col']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67d37b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_mode(train, validate, test, cols, strategy='most_frequent'):\n",
    "    '''\n",
    "    This formula takes in train, validate, and test dataframes, imputes the value specified,\n",
    "    and returns train, validate, and test dataframes\n",
    "    '''\n",
    "    imputer = SimpleImputer(missing_values = None, strategy=strategy)\n",
    "    train[[cols]] = imputer.fit_transform(train[[cols]])\n",
    "    validate[[cols]] = imputer.transform(validate[[cols]])\n",
    "    test[[cols]] = imputer.transform(test[[cols]])\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d40d9bc",
   "metadata": {},
   "source": [
    "## Step 5: Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8402750",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'name': 'student'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd93d0a",
   "metadata": {},
   "source": [
    "## Step 6: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca408f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers_series.apply(lambda n: 'even' if n % 2 == 0 else 'odd')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd91f3e",
   "metadata": {},
   "source": [
    "## Step 7: Encode Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7b3061",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df = pd.get_dummies(df[['col1','col2']], dummy_na=False, drop_first=[True, True])\n",
    "df = pd.concat([df, dummy_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd02097f",
   "metadata": {},
   "source": [
    "## Step 8: Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88e021b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train, validate, and test\n",
    "def split_data(df, random_state=369, stratify=None):\n",
    "    '''\n",
    "    This function takes in a dataframe and splits the data into train, validate and test samples. \n",
    "    Test, validate, and train are 20%, 24%, & 56% of the original dataset, respectively. \n",
    "    The function returns train, validate and test dataframes.\n",
    "    '''\n",
    "    if stratify == None:\n",
    "        # split dataframe 80/20\n",
    "        train_validate, test = train_test_split(df, test_size=.2, random_state=random_state)\n",
    "        # split larger dataframe from previous split 70/30\n",
    "        train, validate = train_test_split(train_validate, test_size=.3, random_state=random_state)\n",
    "    else:\n",
    "        # split dataframe 80/20\n",
    "        train_validate, test = train_test_split(df, test_size=.2, random_state=random_state, stratify=df[stratify])\n",
    "        # split larger dataframe from previous split 70/30\n",
    "        train, validate = train_test_split(train_validate, test_size=.3, \n",
    "                            random_state=random_state,stratify=train_validate[stratify])\n",
    "    # results in 3 dataframes\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96a0308",
   "metadata": {},
   "source": [
    "## Step 9: Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34397a5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a33ad523",
   "metadata": {},
   "source": [
    "# Exploration & Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b0c9a1",
   "metadata": {},
   "source": [
    "**The goal** is to discover features that have the largest impact on the target variable, i.e. provide the most information gain, drive the outcome.\n",
    "\n",
    "**The deliverable** is a file, preprocess.py, that contains the function(s) needed to reproduce the pre-processing of the data. \n",
    "\n",
    "The dataframe resulting from these functions should be one that is pre-processed, i.e. ready to be used in modeling. This means that attributes are reduced to features, features are in a numeric form, there are no missing values, and continuous and/or ordered values are scaled to be unitless.\n",
    "\n",
    "**How to get there:**\n",
    "\n",
    "- Use python libraries: pandas, statsmodels, scipy, numpy, matplotlib, seaborn, scikit-learn.\n",
    "- Perform statistical testing to understand correlations, significant differences in variables, variable interdependencies, etc.\n",
    "- Create visualizations that demonstrate relationships across and within attributes and target.\n",
    "- Use domain knowledge and/or information gained through exploration to construct new features.\n",
    "- Remove features that are noisy, provide no valuable or new information, or are redundant.\n",
    "- Use scikit-learn's preprocessing algorithms (feature selection, feature engineering, dummy variables, binning, clustering, e.g.) to turn attributes into features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b287ce1a",
   "metadata": {},
   "source": [
    "## Step 1: Explore Univariate Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d49c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate stats\n",
    "\n",
    "# to prevent scientific notation where avoidable\n",
    "df.describe().apply(lambda s: s.apply(lambda x: format(x, 'g')))\n",
    "# has option to include or exclude certain dtypes\n",
    "\n",
    "df.col.value_counts(dropna=False, ascending=True).plot.bar()\n",
    "\n",
    "df.groupby('col').col.describe()\n",
    "df[['col1', 'col2', 'col3']].agg(['mean', 'min', 'max'])\n",
    "df.groupby('col').col.agg(['min', 'mean', 'max'])\n",
    "\n",
    "# Use .describe with object columns.\n",
    "obj_cols = df.columns[[df[col].dtype == 'O' for col in df.columns]]\n",
    "for col in obj_cols:\n",
    "    print(df[col].value_counts())\n",
    "    print(df[col].value_counts(normalize=True, dropna=False))\n",
    "    print('----------------------')\n",
    "    \n",
    "# Check out distributions of numeric columns.\n",
    "num_cols = df.columns[[df[col].dtype == 'int64' for col in df.columns]]\n",
    "for col in num_cols:\n",
    "    plt.hist(df[col])\n",
    "    plt.title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1a9b81",
   "metadata": {},
   "source": [
    "## Step 2: Ask questions before exploring further"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952cc276",
   "metadata": {},
   "source": [
    "## Step 3: Explore bivariate stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b5577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplot, catplot, barplot, boxplot\n",
    "# bin data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c6bc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6)) # (width, height)\n",
    "plt.plot(x1, c='green', alpha=0.6)\n",
    "plt.plot(x2, c='red', alpha=0.4)\n",
    "plt.xlim(-20, 200)\n",
    "plt.ylim(-5, 160)\n",
    "plt.title('A couple of random series')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$x^2$')\n",
    "plt.xticks([1, 2, 3.14, 4], ['one', 'two', '$\\pi$', 'four'], rotation=45)\n",
    "plt.text(0.25, 0, '(0, 0)', fontsize=10, color='blue')\n",
    "plt.annotate('Intersection', xy=(0, 0), xytext=(-3, 5),\n",
    "             arrowprops={'facecolor': 'blue'})\n",
    "population_survival_rate = train.survived.mean()\n",
    "plt.axhline(population_survival_rate, label=\"Population survival rate\")\n",
    "# plot the first subplot\n",
    "plt.subplot(n_rows, n_cols, 1)\n",
    "plt.suptitle('Subplots Demo')\n",
    "# Until we call .show, we'll be referring to the same chart, so we can keep adding to it.\n",
    "plt.show()\n",
    "plt.savefig('my-figure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16a7742",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 3))\n",
    "\n",
    "# List of columns\n",
    "cols = ['exam1', 'exam2', 'exam3', 'final_grade']\n",
    "\n",
    "for i, col in enumerate(cols):\n",
    "    \n",
    "    # i starts at 0, but plot nos should start at 1\n",
    "    plot_number = i + 1 \n",
    "    \n",
    "    # Create subplot.\n",
    "    plt.subplot(1,4, plot_number)\n",
    "    \n",
    "    # Title with column name.\n",
    "    plt.title(col)\n",
    "    \n",
    "    # Display histogram for column.\n",
    "    df[col].hist(bins=5, edgecolor='black')\n",
    "    \n",
    "    # Hide gridlines.\n",
    "    plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e026a456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "sns.displot(x='final_grade', data=df)\n",
    "\n",
    "plt.title('final_grade')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "sns.pairplot(df, corner=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af6ce8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.crosstab(tips.time, tips['size'])\n",
    "sns.heatmap(data, annot=True, cmap=plt.cm.Greens)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1263c06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crosstab to view relationship between two variables\n",
    "pd.crosstab(X_train.contract_type, y_train, normalize='index', margins=True).style.background_gradient(cmap='Greens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a17765",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pivot_table(index='classroom', columns='passing_math', values='math')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e669d806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check assumptions: Normal distribution? Independent? Equal variance?\n",
    "\n",
    "# compare observed mean to theoretical mean, numeric to categoricalb\n",
    "t, p = stats.ttest_1samp(sample, overall_mean) # parametric, normal distribution\n",
    "t, p = stats.wilcoxen(sample, overall_mean) # nonparametric\n",
    "# compare two observed means (independent samples)\n",
    "t, p = stats.ttest_ind(sample1, sample2, equal_var=False) # parametric, normal distribution\n",
    "t, p = stats.mannwhitneyu(sample1, sample2) # nonparametric\n",
    "# compare several observed means (independent samples)\n",
    "f, p = stats.f_oneway(sample1, sample2, sample3) # parametric, normal distribution\n",
    "stats.kruskal(sample1, sample2, sample3) # nonparametric\n",
    "# check variance\n",
    "df[col].var()\n",
    "\n",
    "# check numeric to numeric relationship\n",
    "corr, p = stats.pearsonr(x, y) # for linear relationship\n",
    "corr, p = stats.spearmanr(x, y) # for nonlinear relationship\n",
    "\n",
    "# compare two categorical variables\n",
    "chi2, p, degf, expected = stats.chi2_contingency(observed) # observed is a crosstab\n",
    "\n",
    "# \"There is sufficient evidence to move forward understanding that there is a relationship between x and y\"\n",
    "\n",
    "if p < alpha:\n",
    "    print(\"We reject $H_{0}$\")\n",
    "else:\n",
    "    print(\"We fail to reject $H_{0}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fe36c8",
   "metadata": {},
   "source": [
    "## Document findings and takeaways even if it's that there is nothing between x and y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d4d5b9",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d135ba",
   "metadata": {},
   "source": [
    "**The goal** is to create a robust and generalizable model that is a mapping between features and a target outcome.\n",
    "\n",
    "**The deliverable** is a file, model.py, that contains functions for training the model (fit), predicting the target on new data, and evaluating results.\n",
    "\n",
    "**How to get there:**\n",
    "\n",
    "- Python libraries: scikit-learn\n",
    "- Identify regression, classification, cross validataion, and/or other algorithms that are most appropriate.\n",
    "- Build your model:\n",
    "- Create the model object.\n",
    "- Fit the model to your training, or in-sample, observations.\n",
    "- Predict the target value on your training observations.\n",
    "- Evaluate results on the in-sample predictions.\n",
    "- Repeat as necessary with other algorithms or hyperparameters.\n",
    "- Using the best performing model, predict on test, out-of-sample, observations.\n",
    "- Evaluate results on the out-of-sample predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdf3338",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(columns=['cols'])\n",
    "y_train = train.col\n",
    "\n",
    "X_validate = validate.drop(columns=['cols'])\n",
    "y_validate = validate.col\n",
    "\n",
    "X_test = test.drop(columns=['cols'])\n",
    "y_test = test.col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c88519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree classifier\n",
    "clf = DecisionTreeClassifier(max_depth=3, random_state=123)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "# visualize decision tree\n",
    "plt.figure(figsize=(13, 7))\n",
    "plot_tree(clf, feature_names=X_train.columns, class_names=clf.classes_, rounded=True)\n",
    "# make predictions\n",
    "y_pred = clf.predict(X_train)\n",
    "y_pred[0:5]\n",
    "# estimate probability\n",
    "y_pred_proba = clf.predict_proba(X_train)\n",
    "y_pred_proba[0:5]\n",
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "      .format(clf.score(X_train, y_train)))\n",
    "confusion_matrix(y_train, y_pred)\n",
    "print(classification_report(y_train, y_pred))\n",
    "\n",
    "model1 = DecisionTreeClassifier(max_depth=2)\n",
    "model1.fit(X_train[features], y_train)\n",
    "accuracy = model1.score(X_validate[features], y_validate)\n",
    "print(f'Model 1 Accuracy: {accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790e7278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification algorithms\n",
    "\n",
    "# used to predict binary outcomes\n",
    "Logistic Regression (sklearn.linear_model.LogisticRegression)\n",
    "# A sequence of rules used to classify 2 or more classes.\n",
    "Decision Tree (sklearn.tree.DecisionTreeClassifier)\n",
    "Naive Bayes (sklearn.naive_bayes.BernoulliNB)\n",
    "K-Nearest Neighbors (sklearn.neighbors.KNeighborsClassifier)\n",
    "Random Forest (sklearn.ensemble.RandomForestClassifier)\n",
    "Support Vector Machine (sklearn.svm.SVC)\n",
    "Stochastic Gradient Descent (sklearn.linear_model.SGDClassifier)\n",
    "AdaBoost (sklearn.ensemble.AdaBoostClassifier)\n",
    "Bagging (sklearn.ensemble.BaggingClassifier)\n",
    "Gradient Boosting (sklearn.ensemble.GradientBoostingClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d228b4bb",
   "metadata": {},
   "source": [
    "# Delivery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e587595b",
   "metadata": {},
   "source": [
    "**The goal** is to enable others to use what you have learned or developed through all the previous stages.\n",
    "\n",
    "**The deliverable** could be of various types:\n",
    "\n",
    "- A pipeline.py file that takes new observations from acquisition to prediction using the previously built functions.\n",
    "- A fully deployed model.\n",
    "- A reproducible report and/or presentation with recommendations of actions to take based on original project goals.\n",
    "- Predictions made on a specific set of observations.\n",
    "- A dashboard for observing/monitoring the key drivers, or features, of the target variable.\n",
    "\n",
    "**How to get there:**\n",
    "\n",
    "- Python sklearn's pipeline method.\n",
    "- Tableau for creating a report, presentation, story, or dashboard.\n",
    "- Jupyter notebook for creating a report or a framework to reproduce your research, e.g.\n",
    "- Flask to build a web server that provides a gateway to our model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155ba895",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
